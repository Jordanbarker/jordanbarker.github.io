{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://aclanthology.org/D14-1181.pdf] Convolutional Neural Networks for Sentence Classification by Yoon Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: cuda\n",
      "Number of Train Headlines: 19200\n",
      "Number of Test Headlines: 4800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 65-Year-Old Man's Typewriter Was &lt;strong&gt;Des...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can You Identify These 5 UNITED STATES Leaders...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Index of Economic Activity Declined in March\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015's Best News Bloopers Are Here And They're...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18 Pictures Everyone Who Loves Spilling The Te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  label\n",
       "0  A 65-Year-Old Man's Typewriter Was <strong>Des...      1\n",
       "1  Can You Identify These 5 UNITED STATES Leaders...      1\n",
       "2     Index of Economic Activity Declined in March\\n      0\n",
       "3  2015's Best News Bloopers Are Here And They're...      1\n",
       "4  18 Pictures Everyone Who Loves Spilling The Te...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running:\", device)\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(f'Number of Train Headlines: {len(df_train)}')\n",
    "print(f'Number of Test Headlines: {len(df_test)}')\n",
    "df_train.iloc[:,1:].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headline = \"A 65-Year-Old Man's Typewriter Was <strong>Destroyed</strong> By An Angry Cop, And The Internet Got Him A New One &nbsp;\\n\"\n",
    "\n",
    "simple_preprocess(headline) = ['year', 'old', 'man', 'typewriter', 'was', 'strong', 'destroyed', 'strong', 'by', 'an', 'angry', 'cop', 'and', 'the', 'internet', 'got', 'him', 'new', 'one', 'nbsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word2vec_vector(w2v_model, sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            index = w2v_model.wv.key_to_index[word]\n",
    "        except KeyError as e:\n",
    "            index = 0\n",
    "        padded_X[i] = index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device)#.view(1, -1)\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, shuffle=True, batch_size=128, num_workers=1):\n",
    "    return DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=False) # Was throwing errors with True\n",
    "\n",
    "\n",
    "preprocessed_x_train = list(df_train['headline'].apply(simple_preprocess, deacc=True))\n",
    "preprocessed_x_test = list(df_test['headline'].apply(simple_preprocess, deacc=True))\n",
    "w2v_model = Word2Vec(preprocessed_x_train, vector_size=500, min_count=1, workers=3, window=3, sg=1)\n",
    "max_sen_len = max(map(len, preprocessed_x_train))\n",
    "padding_idx = w2v_model.wv.key_to_index['pad']\n",
    "\n",
    "# Make tensor datasets\n",
    "x_train_tensor = torch.stack([make_word2vec_vector(w2v_model, sentence) for sentence in preprocessed_x_train])\n",
    "x_test_tensor = torch.stack([make_word2vec_vector(w2v_model, sentence) for sentence in preprocessed_x_test])\n",
    "y_train_tensor = torch.tensor(df_train['label'], dtype=torch.long, device=device)\n",
    "y_test_tensor = torch.tensor(df_test['label'], dtype=torch.long, device=device)\n",
    "\n",
    "dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validate_size = int(0.2 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, validate_size])\n",
    "train_dl = create_dataloader(train_dataset)\n",
    "val_dl = create_dataloader(val_dataset, shuffle=False)\n",
    "test_dl = create_dataloader(TensorDataset(x_test_tensor, y_test_tensor), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCNClassifier(pl.LightningModule):\n",
    "    def __init__(self, w2vmodel, num_classes=2, window_sizes=(1,2,3,5)):\n",
    "        super().__init__()\n",
    "        self.lr = 0.0005\n",
    "        self.num_classes = num_classes\n",
    "        weights = w2vmodel.wv # used to initialize the embedding layer\n",
    "        EMBEDDIN_GSIZE = 500  # Use this to set the embedding_dim in embedding layer\n",
    "        NUM_FILTERS = 10      # Number of filters in CNN\n",
    "\n",
    "        weights = []\n",
    "        for key in w2vmodel.wv.index_to_key:\n",
    "            weights.append(w2vmodel.wv.get_vector(key))\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        self.emb = nn.Embedding.from_pretrained(weights)\n",
    "\n",
    "        conv_list = []\n",
    "        for window in window_sizes:\n",
    "            conv_list.append(nn.Conv2d(1, NUM_FILTERS, (window, EMBEDDING_SIZE), padding=(window - 1, 0)))\n",
    "        self.convs = nn.ModuleList(conv_list)\n",
    "        \n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = [F.tanh(conv(x).squeeze(2)) for conv in self.convs]  \n",
    "        x = [F.max_pool1d(i, i.size(1)) for i in x] \n",
    "        x = torch.cat(x).squeeze(1)\n",
    "        logits = self.fc(x)\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        # return logits\n",
    "        return probs.unsqueeze(0)\n",
    "\n",
    "    def _common_step(self, batch, type):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        # print(\"logits shape: \", logits.shape)\n",
    "        # print(\"labels shape: \", y.squeeze().shape)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        acc = accuracy(torch.argmax(logits, dim=1), \n",
    "                        y,\n",
    "                        task='multiclass',\n",
    "                        num_classes=self.num_classes)\n",
    "        self.log(f'{type}_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(f'{type}_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"validation\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.0001)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "model = CCNClassifier(w2v_model) \n",
    "model_name = 'CCNClassifier'\n",
    "max_epochs = 5\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# print(\"print wd \", os.getcwd())\n",
    "torch.manual_seed(42) \n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# wandb.init(project='RF-Fingerprinting')\n",
    "# wandb_logger = WandbLogger(name=f\"{model_name}\", save_dir=f\"../Data/Logs/{model_name}\")\n",
    "# csv_logger = CSVLogger(save_dir=f\"../Data/Logs/{model_name}\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    # logger=[wandb_logger, csv_logger],\n",
    "    # enable_checkpointing=False\n",
    "    # log_every_n_steps=10000\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, dataloaders=val_dl)\n",
    "# trainer.test(model, dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCNClassifier(pl.LightningModule):\n",
    "    def __init__(self, w2vmodel, num_classes=2, cnn_filters=10, window_sizes=(1,2,3,5), emb_size=500):\n",
    "        super().__init__()\n",
    "        self.lr = 0.0005\n",
    "        self.num_classes = num_classes\n",
    "        weights = torch.FloatTensor([w2vmodel.wv.get_vector(key) for key in w2vmodel.wv.index_to_key])\n",
    "        self.emb = nn.Embedding.from_pretrained(weights)\n",
    "        conv_list = [nn.Conv2d(1, cnn_filters, (window, emb_size), padding=(window - 1, 0)) for window in window_sizes]\n",
    "        self.convs = nn.ModuleList(conv_list)\n",
    "        self.fc = nn.Linear(cnn_filters * len(window_sizes), self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x).unsqueeze(1) # add a channel dimension\n",
    "        x = [F.relu(conv(x).squeeze(3)) for conv in self.convs]  \n",
    "        x = [F.max_pool1d(i, i.size(2)) for i in x] \n",
    "        x = torch.cat(x, dim=2).squeeze(1)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def _common_step(self, batch, type):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y.squeeze())\n",
    "        acc = accuracy(torch.argmax(logits).view(-1), \n",
    "                        y,\n",
    "                        task='binary',\n",
    "                        num_classes=self.num_classes)\n",
    "        self.log(f'{type}_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(f'{type}_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"validation\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric_\\AppData\\Local\\Temp\\ipykernel_10320\\2512316038.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  weights = torch.FloatTensor([w2vmodel.wv.get_vector(key) for key in w2vmodel.wv.index_to_key])\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | emb   | Embedding  | 8.5 M \n",
      "1 | convs | ModuleList | 55.0 K\n",
      "2 | fc    | Linear     | 82    \n",
      "-------------------------------------\n",
      "55.1 K    Trainable params\n",
      "8.5 M     Non-trainable params\n",
      "8.6 M     Total params\n",
      "34.418    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0ee0cd4598444da34180f807bd5d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1280x4 and 40x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m max_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mmax_epochs)\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dl, val_dataloaders\u001b[39m=\u001b[39;49mval_dl)\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    529\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 531\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    533\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    561\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    562\u001b[0m )\n\u001b[0;32m    564\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    566\u001b[0m     ckpt_path,\n\u001b[0;32m    567\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m )\n\u001b[1;32m--> 570\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1016\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1017\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1018\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1045\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1042\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1044\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1045\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1047\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1049\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:287\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    290\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:379\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m    378\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mCCNClassifier.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_common_step(batch, \u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mCCNClassifier._common_step\u001b[1;34m(self, batch, type)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_common_step\u001b[39m(\u001b[39mself\u001b[39m, batch, \u001b[39mtype\u001b[39m):\n\u001b[0;32m     21\u001b[0m     x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> 22\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\n\u001b[0;32m     23\u001b[0m     loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(logits, y\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m     24\u001b[0m     acc \u001b[39m=\u001b[39m accuracy(torch\u001b[39m.\u001b[39margmax(logits)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \n\u001b[0;32m     25\u001b[0m                     y,\n\u001b[0;32m     26\u001b[0m                     task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m                     num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mCCNClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mmax_pool1d(i, i\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x] \n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(x, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1280x4 and 40x2)"
     ]
    }
   ],
   "source": [
    "model = CCNClassifier(w2v_model)\n",
    "max_epochs = 1\n",
    "trainer = pl.Trainer(max_epochs=max_epochs)\n",
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95059ba4be4c49519df8f6ec0f107dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.50020831823349      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7622038125991821     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.50020831823349     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7622038125991821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7622038125991821, 'test_accuracy': 0.50020831823349}]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "window_sizes = (1,2,3,5)\n",
    "num_classes = 2\n",
    "cnn_filters = 10\n",
    "emb_size = 500\n",
    "weights = torch.FloatTensor(np.array([w2v_model.wv.get_vector(key) for key in w2v_model.wv.index_to_key]))\n",
    "emb = nn.Embedding.from_pretrained(weights).to(device)\n",
    "conv_list = [nn.Conv2d(1, cnn_filters, (window, emb_size), padding=(window - 1, 0)) for window in window_sizes]\n",
    "convs = nn.ModuleList(conv_list).to(device)\n",
    "fc = nn.Linear(cnn_filters * len(window_sizes), num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17099, 500])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(np.array([w2v_model.wv.get_vector(key) for key in w2v_model.wv.index_to_key])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "x.shape, y.shape # (torch.Size([batch_size, 22]), torch.Size([batch_size]))\n",
    "\n",
    "x = emb(x)         # torch.Size([batch_size, 22, 500])\n",
    "x = x.unsqueeze(1) # torch.Size([batch_size, 1, 22, 500])\n",
    "x = [F.relu(conv(x).squeeze(3)) for conv in convs]\n",
    "x = [F.max_pool1d(i, i.size(2)) for i in x] \n",
    "# x = torch.cat(x, dim=1).squeeze(1) # torch.Size([batch_size, 10, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40, 1])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(x, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat(x, dim=2).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5120x1 and 40x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[318], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fc(torch\u001b[39m.\u001b[39;49mcat(x, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\eric_\\Documents\\Code\\py_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5120x1 and 40x2)"
     ]
    }
   ],
   "source": [
    "fc(torch.cat(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.emb(x).unsqueeze(1) # add a channel dimension\n",
    "x = [F.relu(conv(x).squeeze(3)) for conv in self.convs] \n",
    "x = [F.max_pool1d(i, i.size(2)) for i in x] \n",
    "x = torch.cat(x, dim=2).squeeze(1)\n",
    "logits = self.fc(x)\n",
    "return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 22, 500])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 10, 22, 1]), torch.Size([128, 10, 23, 1]))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convs[0](x).shape, convs[1](x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 500])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = self(x)\n",
    "# print(\"logits shape: \", logits.shape)\n",
    "# print(\"x shape\", x.shape)\n",
    "# print(\"y shape\", y.shape)\n",
    "# print(\"logits\", logits)\n",
    "# print(\"y\", y)\n",
    "# print(\"labels shape: \", y.squeeze().shape)\n",
    "loss = nn.CrossEntropyLoss()(logits, y.squeeze())\n",
    "acc = accuracy(torch.argmax(logits).view(-1), \n",
    "                y,\n",
    "                task='binary',\n",
    "                num_classes=self.num_classes)\n",
    "self.log(f'{type}_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "self.log(f'{type}_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 156, 6381,    0,   76,  487, 1284, 1627,    2,   78, 3766, 3766, 3766,\n",
       "          3766, 3766, 3766, 3766, 3766, 3766, 3766, 3766, 3766, 3766]]),\n",
       " tensor([0])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = self.\n",
    "x = [F.tanh(conv(x).squeeze(2)) for conv in self.convs]  \n",
    "x = [F.max_pool1d(i, i.size(1)) for i in x] \n",
    "x = torch.cat(x).squeeze(1)\n",
    "logits = self.fc(x)\n",
    "return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
