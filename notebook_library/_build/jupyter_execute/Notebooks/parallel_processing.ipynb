{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing\n",
    "\n",
    "## Overview\n",
    "\n",
    "### The GIL (Global Interpreter Lock) in Python\n",
    "Python's GIL allows only one thread to execute at a time in a single process, affecting CPU-bound threading performance.\n",
    "\n",
    "### Parallelism vs. Concurrency\n",
    "- Parallelism: Performing multiple tasks simultaneously.\n",
    "- Concurrency: Managing multiple tasks at the same time but not necessarily simultaneously.\n",
    "\n",
    "### Challenges\n",
    "- Data Race: Two threads access shared data simultaneously, causing inconsistent results.\n",
    "- Deadlock: Two or more threads wait indefinitely for resources held by each other.\n",
    "- Thread Safety: Writing code that functions correctly during simultaneous execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parallel Processing with `joblib`\n",
    "\n",
    "`joblib` provides a simple interface for efficient parallel computing.\n",
    "\n",
    "\n",
    "`Parallel(n_jobs=-1)` will automatically distribute the workload across all available CPU cores.\n",
    "\n",
    "Here is a basic example, where we need to call `time_intensive_method()` several times. Doing this in parallel streamlines it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def time_intensive_method(n):\n",
    "    time.sleep(1)\n",
    "    return n\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(time_intensive_method)(n) for n in range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing with `multiprocessing`\n",
    "\n",
    "```{note}\n",
    "Special considerations apply when using Jupyter or Windows for this. Check out  \n",
    "[Bob Swinkels's blog on the topic](https://bobswinkels.com/posts/multiprocessing-python-windows-jupyter/) for details.\n",
    "```\n",
    "\n",
    "Creating Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def worker(num):\n",
    "    print(f'Worker {num}')\n",
    "\n",
    "processes = []\n",
    "for i in range(5):\n",
    "    p = Process(target=worker, args=(i,))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    \n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize the computation using `Pool` to create 4 worker processes. This distributes the function across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "with Pool(4) as pool:\n",
    "    result = pool.map(square, [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication Between Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Queue` allows us to communicate between processes. \n",
    "\n",
    "In this example, a new process starts with the queue as an argument. Inside the worker function, `q.put('Hello from worker')` adds a message to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def worker(q):\n",
    "    q.put('Hello from worker') # Add message to the queue\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue()\n",
    "    p = Process(target=worker, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get())  # Output: Hello from worker\n",
    "    p.join() # Ensures the worker process completes before exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronizing Processes\n",
    "Locks, Semaphores, Events, and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def printer(item, lock):\n",
    "    with lock:\n",
    "        print(item)\n",
    "\n",
    "lock = Lock()\n",
    "items = ['A', 'B', 'C']\n",
    "for item in items:\n",
    "    Process(target=printer, args=(item, lock)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Shared Memory\n",
    "We can use `Value` and `Array` to share memory across processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Value, Array\n",
    "\n",
    "def increment(value):\n",
    "    value.value += 1\n",
    "\n",
    "num = Value('i', 0)\n",
    "p = Process(target=increment, args=(num,))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how it would look with `Array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(array):\n",
    "    for i in range(len(array)):\n",
    "        array[i] += 1\n",
    "\n",
    "nums = Array('i', [0, 1, 2, 3])  # Shared array of integers\n",
    "p = Process(target=increment, args=(nums,))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async\n",
    "Async programming allows for concurrent code execution using coroutines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def say_hello():\n",
    "    print('Hello')\n",
    "    await asyncio.sleep(1)\n",
    "    print('World')\n",
    "\n",
    "asyncio.run(say_hello())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example to perform asynchronous HTTP requests, which allows other tasks to proceed while waiting for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, 'https://example.com')\n",
    "        print(html)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics\n",
    "- Swifter: Parallel Data Processing with Pandas [(Github page)](https://github.com/jmcarpenter2/swifter)\n",
    "- Dask enables parallel computing with task scheduling and can handle datasets larger than memory.\n",
    "- Numba and CuPy for GPU parallel processing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}